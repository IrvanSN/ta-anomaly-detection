{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('drive/MyDrive/Colab Notebooks/dataset/system_stats.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "print(\"\\nStatistical summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of features\n",
    "plt.figure(figsize=(18, 12))\n",
    "features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    sns.histplot(df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series plot of key features\n",
    "plt.figure(figsize=(15, 8))\n",
    "for feature in features[:5]:  # Plot first 5 features\n",
    "    plt.plot(df[feature], label=feature)\n",
    "plt.legend()\n",
    "plt.title('Time Series of Key Features')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "corr = df.select_dtypes(include=['float64', 'int64']).corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', \n",
    "            vmin=-1, vmax=1, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select numerical features for modeling\n",
    "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=numerical_df.columns)\n",
    "\n",
    "# Split data into training and testing sets (80/20)\n",
    "train_data, test_data = train_test_split(scaled_df, test_size=0.2, \n",
    "                                         shuffle=False)  # No shuffle for time series\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the autoencoder architecture\n",
    "input_dim = train_data.shape[1]\n",
    "\n",
    "# Build the autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(int(input_dim * 0.75), activation=\"relu\")(input_layer)\n",
    "    encoder = Dense(int(input_dim * 0.5), activation=\"relu\")(encoder)\n",
    "    encoder = Dense(int(input_dim * 0.33), activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Bottleneck layer\n",
    "    bottleneck = Dense(int(input_dim * 0.25), activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = Dense(int(input_dim * 0.33), activation=\"relu\")(bottleneck)\n",
    "    decoder = Dense(int(input_dim * 0.5), activation=\"relu\")(decoder)\n",
    "    decoder = Dense(int(input_dim * 0.75), activation=\"relu\")(decoder)\n",
    "    output_layer = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Build and train the autoencoder\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "autoencoder.summary()\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, \n",
    "                              restore_best_weights=True)\n",
    "\n",
    "# Train the autoencoder\n",
    "history_autoencoder = autoencoder.fit(\n",
    "    train_data, train_data,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(test_data, test_data),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
    "plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on test data\n",
    "autoencoder_test_pred = autoencoder.predict(test_data)\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "test_mae = np.mean(np.abs(test_data.values - autoencoder_test_pred), axis=1)\n",
    "test_mse = np.mean(np.square(test_data.values - autoencoder_test_pred), axis=1)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_mae = mean_absolute_error(test_data, autoencoder_test_pred)\n",
    "overall_mse = mean_squared_error(test_data, autoencoder_test_pred)\n",
    "overall_rmse = math.sqrt(overall_mse)\n",
    "\n",
    "print(f\"Autoencoder Evaluation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {overall_mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {overall_rmse:.4f}\")\n",
    "\n",
    "# Visualize the reconstruction error\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(test_mae, bins=50, alpha=0.75)\n",
    "plt.axvline(x=np.percentile(test_mae, 95), color='r', linestyle='--', \n",
    "            label='95th Percentile Threshold')\n",
    "plt.title('Distribution of Reconstruction Errors (MAE)')\n",
    "plt.xlabel('MAE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize actual vs reconstructed for a sample of features\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, col in enumerate(numerical_df.columns[:6]):  # Plot first 6 features\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(test_data.iloc[:, i], autoencoder_test_pred[:, i], alpha=0.5)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')  # Perfect reconstruction line\n",
    "    plt.title(f'Actual vs Reconstructed: {col}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Reconstructed')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Set a threshold for anomaly detection (95th percentile of reconstruction error)\n",
    "threshold = np.percentile(test_mae, 95)\n",
    "print(f\"Anomaly threshold: {threshold:.4f}\")\n",
    "\n",
    "# Flag anomalies\n",
    "anomalies = test_mae > threshold\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "print(f\"Percentage of anomalies: {np.sum(anomalies) / len(test_mae) * 100:.2f}%\")\n",
    "\n",
    "# Visualize anomalies over time for a key feature\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(range(len(test_data)), test_data.iloc[:, 0], c=['blue' if not a else 'red' for a in anomalies], alpha=0.5)\n",
    "plt.title(f'Anomaly Detection for {test_data.columns[0]}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend(['Normal', 'Anomaly'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the autoencoder model\n",
    "autoencoder.save('drive/MyDrive/Colab Notebooks/models/autoencoder_model.h5')\n",
    "np.save('drive/MyDrive/Colab Notebooks/models/anomaly_threshold.npy', threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create labels for LSTM (using autoencoder's anomaly detection as pseudo-labels)\n",
    "# In a real scenario, you would use actual labeled data\n",
    "y_pseudo = anomalies.astype(int)\n",
    "\n",
    "# LSTM requires sequence data\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data.iloc[i:(i + seq_length)].values\n",
    "        y = y_pseudo[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Define sequence length\n",
    "seq_length = 10  # Look back 10 time steps\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X_seq, y_seq = create_sequences(test_data.reset_index(drop=True), seq_length)\n",
    "\n",
    "print(f\"LSTM input shape: {X_seq.shape}\")\n",
    "print(f\"LSTM output shape: {y_seq.shape}\")\n",
    "\n",
    "# Split the sequence data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, \n",
    "                                                 random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(seq_length, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(seq_length, n_features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and train LSTM model\n",
    "lstm_model = build_lstm_model(seq_length, X_train.shape[2])\n",
    "lstm_model.summary()\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, \n",
    "                              restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_lstm.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('LSTM Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_proba = lstm_model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"LSTM Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the LSTM model\n",
    "lstm_model.save('drive/MyDrive/Colab Notebooks/models/lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function for real-time anomaly detection\n",
    "def detect_anomalies_realtime(new_data, autoencoder, threshold, \n",
    "                             lstm_model=None, sequence_buffer=None, \n",
    "                             seq_length=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Detect anomalies in real-time data\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data: raw new data point (numpy array)\n",
    "    - autoencoder: trained autoencoder model\n",
    "    - threshold: anomaly threshold\n",
    "    - lstm_model: trained LSTM model (optional)\n",
    "    - sequence_buffer: buffer containing recent data points for LSTM (optional)\n",
    "    - seq_length: sequence length for LSTM (optional)\n",
    "    - scaler: fitted scaler for data normalization\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with detection results\n",
    "    \"\"\"\n",
    "    # Preprocess the data (normalize)\n",
    "    if scaler is not None:\n",
    "        new_data_scaled = scaler.transform(new_data.reshape(1, -1))\n",
    "    else:\n",
    "        new_data_scaled = new_data.reshape(1, -1)\n",
    "    \n",
    "    # Get autoencoder prediction\n",
    "    reconstruction = autoencoder.predict(new_data_scaled, verbose=0)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    reconstruction_error = np.mean(np.abs(new_data_scaled - reconstruction))\n",
    "    \n",
    "    # Detect anomaly with autoencoder\n",
    "    is_anomaly_autoencoder = reconstruction_error > threshold\n",
    "    \n",
    "    result = {\n",
    "        'is_anomaly': bool(is_anomaly_autoencoder),\n",
    "        'reconstruction_error': float(reconstruction_error),\n",
    "        'threshold': float(threshold),\n",
    "        'detection_method': 'autoencoder'\n",
    "    }\n",
    "    \n",
    "    # If LSTM model is provided and we have enough data in the buffer\n",
    "    if (lstm_model is not None and sequence_buffer is not None and \n",
    "            len(sequence_buffer) >= seq_length):\n",
    "        \n",
    "        # Update buffer with new data\n",
    "        sequence_buffer.append(new_data_scaled[0])\n",
    "        if len(sequence_buffer) > seq_length:\n",
    "            sequence_buffer.pop(0)  # Remove oldest data point\n",
    "        \n",
    "        # Prepare sequence for LSTM\n",
    "        lstm_input = np.array([sequence_buffer[-seq_length:]])\n",
    "        \n",
    "        # Get LSTM prediction\n",
    "        lstm_prediction = lstm_model.predict(lstm_input, verbose=0)\n",
    "        is_anomaly_lstm = lstm_prediction[0][0] > 0.5\n",
    "        \n",
    "        # Update result with LSTM information\n",
    "        result['is_anomaly_lstm'] = bool(is_anomaly_lstm)\n",
    "        result['lstm_confidence'] = float(lstm_prediction[0][0])\n",
    "        result['detection_method'] = 'ensemble'\n",
    "        \n",
    "        # Final decision (can be customized)\n",
    "        result['is_anomaly'] = result['is_anomaly'] or result['is_anomaly_lstm']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example of how to use the function in a real-time environment\n",
    "print(\"\\nReal-time Anomaly Detection Example:\")\n",
    "print(\"1. Load the saved models:\")\n",
    "print(\"   autoencoder = load_model('drive/MyDrive/Colab Notebooks/models/autoencoder_model.h5')\")\n",
    "print(\"   lstm_model = load_model('drive/MyDrive/Colab Notebooks/models/lstm_model.h5')\")\n",
    "print(\"   threshold = np.load('drive/MyDrive/Colab Notebooks/models/anomaly_threshold.npy')\")\n",
    "print(\"\\n2. Initialize a buffer for storing recent data points:\")\n",
    "print(\"   sequence_buffer = []\")\n",
    "print(\"\\n3. Every 15 seconds, fetch new data and detect anomalies:\")\n",
    "print(\"   while True:\")\n",
    "print(\"       new_data = fetch_new_data()  # Your function to get fresh data\")\n",
    "print(\"       result = detect_anomalies_realtime(new_data, autoencoder, threshold,\")\n",
    "print(\"                                         lstm_model, sequence_buffer, seq_length, scaler)\")\n",
    "print(\"       if result['is_anomaly']:\")\n",
    "print(\"           send_alert(result)\")\n",
    "print(\"       time.sleep(15)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
