{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from google.colab import drive\n",
    "import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define headers for the dataset\n",
    "header = [\n",
    "    \"timestamp\",\n",
    "    \"cpu_usage\",\n",
    "    \"top_1_cpu_proc_name\",\n",
    "    \"top_1_cpu_proc_usage\",\n",
    "    \"top_2_cpu_proc_name\",\n",
    "    \"top_2_cpu_proc_usage\",\n",
    "    \"top_3_cpu_proc_name\",\n",
    "    \"top_3_cpu_proc_usage\",\n",
    "    \"top_4_cpu_proc_name\",\n",
    "    \"top_4_cpu_proc_usage\",\n",
    "    \"top_5_cpu_proc_name\",\n",
    "    \"top_5_cpu_proc_usage\",\n",
    "    \"mem_usage\",\n",
    "    \"top_1_mem_proc_name\",\n",
    "    \"top_1_mem_proc_usage\",\n",
    "    \"top_2_mem_proc_name\",\n",
    "    \"top_2_mem_proc_usage\",\n",
    "    \"top_3_mem_proc_name\",\n",
    "    \"top_3_mem_proc_usage\",\n",
    "    \"top_4_mem_proc_name\",\n",
    "    \"top_4_mem_proc_usage\",\n",
    "    \"top_5_mem_proc_name\",\n",
    "    \"top_5_mem_proc_usage\",\n",
    "    \"nginx_active_connections\",\n",
    "    \"nginx_rps\"\n",
    "]\n",
    "\n",
    "# Load the dataset with semicolon delimiter\n",
    "df = pd.read_csv(\n",
    "    \"drive/MyDrive/Colab Notebooks/dataset/system_stats.csv\",\n",
    "    header=None, \n",
    "    names=header, \n",
    "    sep=';',\n",
    "    low_memory=False,\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()\n",
    "\n",
    "# Handle timestamp parsing\n",
    "# First, clean up any extra characters that might interfere with parsing\n",
    "df['timestamp'] = df['timestamp'].str.replace('T', ' ')\n",
    "df['timestamp'] = df['timestamp'].str.split('.').str[0]\n",
    "\n",
    "# Now try to parse timestamps\n",
    "try:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "except:\n",
    "    print(\"Warning: Some timestamp parsing failed, using a more flexible approach\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "# Drop rows with invalid timestamps\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "print(f\"Dataset shape after cleaning timestamps: {df.shape}\")\n",
    "\n",
    "# Extract time features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['day_of_month'] = df['timestamp'].dt.day\n",
    "\n",
    "# Check data types and convert numerical columns from strings to float\n",
    "numeric_cols = [\n",
    "    'cpu_usage', 'top_1_cpu_proc_usage', 'top_2_cpu_proc_usage', \n",
    "    'top_3_cpu_proc_usage', 'top_4_cpu_proc_usage', 'top_5_cpu_proc_usage',\n",
    "    'mem_usage', 'top_1_mem_proc_usage', 'top_2_mem_proc_usage', \n",
    "    'top_3_mem_proc_usage', 'top_4_mem_proc_usage', 'top_5_mem_proc_usage',\n",
    "    'nginx_active_connections', 'nginx_rps'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Select only numerical features for the autoencoder (excluding process names)\n",
    "numerical_features = [\n",
    "    'cpu_usage', 'top_1_cpu_proc_usage', 'top_2_cpu_proc_usage', \n",
    "    'top_3_cpu_proc_usage', 'top_4_cpu_proc_usage', 'top_5_cpu_proc_usage',\n",
    "    'mem_usage', 'top_1_mem_proc_usage', 'top_2_mem_proc_usage', \n",
    "    'top_3_mem_proc_usage', 'top_4_mem_proc_usage', 'top_5_mem_proc_usage',\n",
    "    'nginx_active_connections', 'nginx_rps',\n",
    "    'hour', 'day_of_week', 'day_of_month'\n",
    "]\n",
    "\n",
    "# These are the features we'll use for training\n",
    "features = numerical_features.copy()\n",
    "\n",
    "# Check for and handle missing values\n",
    "print(f\"Missing values:\\n{df[features].isna().sum()}\")\n",
    "df[features] = df[features].fillna(0)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), columns=features)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Define the encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Encoder layers\n",
    "encoder = Dense(64, activation='relu')(input_layer)\n",
    "encoder = Dropout(0.2)(encoder)\n",
    "encoder = Dense(32, activation='relu')(encoder)\n",
    "encoder = Dropout(0.2)(encoder)\n",
    "\n",
    "# Bottleneck layer\n",
    "bottleneck = Dense(16, activation='relu')(encoder)\n",
    "\n",
    "# Decoder layers\n",
    "decoder = Dense(32, activation='relu')(bottleneck)\n",
    "decoder = Dropout(0.2)(decoder)\n",
    "decoder = Dense(64, activation='relu')(decoder)\n",
    "decoder = Dropout(0.2)(decoder)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "# Create model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, X_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Training History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction error on the entire dataset\n",
    "reconstructed = autoencoder.predict(df_scaled)\n",
    "mse = np.mean(np.power(df_scaled - reconstructed, 2), axis=1)\n",
    "\n",
    "# Add reconstruction error to original dataframe\n",
    "df['reconstruction_error'] = mse\n",
    "\n",
    "# Determine anomaly threshold (you can adjust this based on your needs)\n",
    "# Here we use mean + 2*std as threshold\n",
    "threshold = np.mean(mse) + 2 * np.std(mse)\n",
    "print(f\"Anomaly threshold: {threshold}\")\n",
    "\n",
    "# Flag anomalies\n",
    "df['is_anomaly'] = df['reconstruction_error'] > threshold\n",
    "anomaly_count = df['is_anomaly'].sum()\n",
    "print(f\"Number of anomalies detected: {anomaly_count} ({anomaly_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Plot reconstruction error\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['timestamp'], df['reconstruction_error'], label='Reconstruction Error')\n",
    "plt.axhline(y=threshold, color='r', linestyle='-', label=f'Threshold ({threshold:.4f})')\n",
    "plt.fill_between(df['timestamp'], threshold, df['reconstruction_error'], \n",
    "                 where=(df['reconstruction_error'] > threshold), color='red', alpha=0.3)\n",
    "plt.title('Reconstruction Error Over Time')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Reconstruction Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display anomalies with highest reconstruction error\n",
    "top_anomalies = df[df['is_anomaly']].sort_values('reconstruction_error', ascending=False).head(10)\n",
    "print(\"Top 10 anomalies by reconstruction error:\")\n",
    "display(top_anomalies[['timestamp', 'cpu_usage', 'mem_usage', 'nginx_rps', 'reconstruction_error']])\n",
    "\n",
    "# Feature contribution to anomalies\n",
    "if anomaly_count > 0:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    anomaly_indices = df[df['is_anomaly']].index\n",
    "    feature_errors = np.power(df_scaled.iloc[anomaly_indices].values - \n",
    "                            reconstructed[anomaly_indices], 2)\n",
    "    feature_error_df = pd.DataFrame(feature_errors, columns=features)\n",
    "    feature_contribution = feature_error_df.mean().sort_values(ascending=False)\n",
    "    \n",
    "    plt.bar(feature_contribution.index, feature_contribution.values)\n",
    "    plt.title('Feature Contribution to Anomalies')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Average Squared Error')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize normal vs anomalous data points for top contributing features\n",
    "if anomaly_count > 0:\n",
    "    top_features = feature_contribution.head(6).index\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        ax = axes[i]\n",
    "        sns.histplot(data=df, x=feature, hue='is_anomaly', bins=30, ax=ax, \n",
    "                   palette={True: 'red', False: 'blue'}, alpha=0.5, legend=i==0)\n",
    "        ax.set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save the model if needed\n",
    "autoencoder.save('/content/drive/MyDrive/Colab Notebooks/system_anomaly_detector.h5')\n",
    "print(\"Model saved to Google Drive\")\n",
    "\n",
    "# Save the full results with all original features plus anomaly detection columns\n",
    "# Convert boolean to string \"True\" or \"False\" for better readability in CSV\n",
    "df['is_anomaly'] = df['is_anomaly'].astype(str)\n",
    "\n",
    "# Save all original columns plus reconstruction_error and is_anomaly\n",
    "# Remove temporary columns we added for analysis\n",
    "columns_to_drop = ['hour', 'day_of_week', 'day_of_month']\n",
    "results_df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('/content/drive/MyDrive/Colab Notebooks/system_anomaly_full_results.csv', index=False)\n",
    "print(\"Full results with original features and anomaly flags saved to Google Drive\")\n",
    "\n",
    "# Also save a focused version with just the anomalies\n",
    "anomaly_df = results_df[results_df['is_anomaly'] == 'True']\n",
    "anomaly_df.to_csv('/content/drive/MyDrive/Colab Notebooks/system_anomalies.csv', index=False)\n",
    "print(f\"Saved {len(anomaly_df)} anomalies to a separate file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Building on our existing autoencoder code\n",
    "# This would be implemented after the autoencoder section\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "\n",
    "# Create sequences from your time series data\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "# Define sequence length (window size)\n",
    "seq_length = 24  # For example, 24 data points (could be 24 hours if hourly data)\n",
    "\n",
    "# Use the same features from our autoencoder\n",
    "lstm_features = features.copy()\n",
    "\n",
    "# Sort by timestamp to ensure proper sequence\n",
    "df_sorted = df.sort_values('timestamp')\n",
    "sequence_data = df_sorted[lstm_features].values\n",
    "\n",
    "# Scale the data using the same scaler as the autoencoder\n",
    "sequence_data_scaled = scaler.transform(sequence_data)\n",
    "\n",
    "# Create sequences\n",
    "X_seq = create_sequences(sequence_data_scaled, seq_length)\n",
    "\n",
    "# Split data\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train_seq = X_seq[:train_size]\n",
    "X_test_seq = X_seq[train_size:]\n",
    "\n",
    "# Build LSTM Autoencoder model\n",
    "lstm_autoencoder = Sequential([\n",
    "    # Encoder\n",
    "    LSTM(64, activation='relu', input_shape=(seq_length, len(lstm_features)), return_sequences=False),\n",
    "    \n",
    "    # Representation\n",
    "    RepeatVector(seq_length),\n",
    "    \n",
    "    # Decoder\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(len(lstm_features)))\n",
    "])\n",
    "\n",
    "lstm_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "lstm_history = lstm_autoencoder.fit(\n",
    "    X_train_seq, X_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_seq, X_test_seq),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "X_test_pred = lstm_autoencoder.predict(X_test_seq)\n",
    "\n",
    "# Calculate MSE for each sequence\n",
    "lstm_mse = np.mean(np.square(X_test_seq - X_test_pred), axis=(1, 2))\n",
    "\n",
    "# Determine threshold\n",
    "lstm_threshold = np.mean(lstm_mse) + 2 * np.std(lstm_mse)\n",
    "print(f\"LSTM Anomaly threshold: {lstm_threshold}\")\n",
    "\n",
    "# Function to detect anomalies in new data\n",
    "def detect_anomalies_with_lstm(new_data, model, scaler, threshold, seq_length, features):\n",
    "    \"\"\"\n",
    "    Detect anomalies in new time series data using the trained LSTM autoencoder\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data: DataFrame with new system metrics\n",
    "    - model: Trained LSTM autoencoder model\n",
    "    - scaler: Fitted scaler\n",
    "    - threshold: Anomaly threshold\n",
    "    - seq_length: Sequence length used for training\n",
    "    - features: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with anomaly scores and flags\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    new_data_scaled = scaler.transform(new_data[features].values)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = create_sequences(new_data_scaled, seq_length)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Warning: Not enough data points for sequence creation\")\n",
    "        return new_data.copy()\n",
    "    \n",
    "    # Get predictions\n",
    "    reconstructions = model.predict(sequences)\n",
    "    \n",
    "    # Calculate errors\n",
    "    mse = np.mean(np.square(sequences - reconstructions), axis=(1, 2))\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = new_data.iloc[seq_length:].copy()\n",
    "    result['lstm_reconstruction_error'] = mse\n",
    "    result['lstm_is_anomaly'] = result['lstm_reconstruction_error'] > threshold\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Save LSTM model for future use\n",
    "lstm_autoencoder.save('/content/drive/MyDrive/Colab Notebooks/system_lstm_anomaly_detector.h5')\n",
    "print(\"LSTM model saved to Google Drive\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
